{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"03_Modelling.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"GiLfgxmYsujm"},"source":["<img src=\"../assets/a_eyes_readme.gif\" style=\"float:right ; margin: 10px ; width:300px;\"> \n","\n","<h1><left>GOODBYE WORLD: using Natural Language Processing to identify suicidal posts</left></h1>\n","<h4><left></left></h4>\n","\n","___\n","\n","## **3. Modelling**\n","\n","In this section, we will be using a Pipeline to score different classifier models like K-Nearest Neighbours and Multinomial Naive Bayes before finally settling on a final production model."]},{"cell_type":"code","metadata":{"id":"f2e8_Zdfsujo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605063576473,"user_tz":-330,"elapsed":7339,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"4af01c1a-65e3-45cc-e9b0-308f8642cee9"},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","pd.set_option('display.max_columns', 100)\n","sns.set_style(\"white\")\n","\n","from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n","\n","# import warnings filter\n","from warnings import simplefilter\n","# ignore all future warnings\n","simplefilter(action='ignore', category=FutureWarning)\n","!pip install wordninja\n","# IF YOU ARE MISSING \"WordCloud\":\n","# TRY INSTALLING VIA TERMINAL LIKE THIS: /anaconda3/bin/python -m pip install wordcloud\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","from PIL import Image\n","# IF YOU ARE MISSING \"wordninja\":\n","# TRY INSTALLING VIA TERMINAL LIKE THIS: pip install wordninja\n","import wordninja"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wordninja\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n","\r\u001b[K     |▋                               | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 1.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 409kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 481kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 501kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 522kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 2.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: wordninja\n","  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wordninja: filename=wordninja-2.0.0-cp36-none-any.whl size=541553 sha256=a058b7fd338235bdc35640c76205247d981c065198cd7d708ce413bb3e7ff477\n","  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n","Successfully built wordninja\n","Installing collected packages: wordninja\n","Successfully installed wordninja-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"evc3TBZY7XyA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622289025115,"user_tz":-330,"elapsed":31782,"user":{"displayName":"Parth Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gii3yHCCm5yzS6M5J3Bgkg7GGDDP3j6r_qq-K0QQA=s64","userId":"12868512136897411496"}},"outputId":"91189a99-a68f-4d22-d0fe-9651104adff9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8fbKauHAsuju"},"source":["model_data = pd.read_csv('/content/drive/My Drive/ML mini project (Group-9) /data_for_model.csv', keep_default_na=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfDoFbcisujz","colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"status":"ok","timestamp":1605063697538,"user_tz":-330,"elapsed":3127,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"256f3497-2568-4370-83c0-87e36b3e11da"},"source":["model_data.head(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>selftext</th>\n","      <th>author</th>\n","      <th>num_comments</th>\n","      <th>is_suicide</th>\n","      <th>url</th>\n","      <th>selftext_clean</th>\n","      <th>title_clean</th>\n","      <th>author_clean</th>\n","      <th>selftext_length</th>\n","      <th>title_length</th>\n","      <th>megatext_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Our most-broken and least-understood rules is ...</td>\n","      <td>We understand that most people who reply immed...</td>\n","      <td>SQLwitch</td>\n","      <td>175</td>\n","      <td>0</td>\n","      <td>https://www.reddit.com/r/depression/comments/d...</td>\n","      <td>understand people reply immediately op invitat...</td>\n","      <td>broken least understood rule helper may invite...</td>\n","      <td>sql witch</td>\n","      <td>4792</td>\n","      <td>144</td>\n","      <td>sql witch understand people reply immediately ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Regular Check-In Post. Plus, a reminder about ...</td>\n","      <td>Welcome to /r/depression's check-in post - a p...</td>\n","      <td>SQLwitch</td>\n","      <td>1237</td>\n","      <td>0</td>\n","      <td>https://www.reddit.com/r/depression/comments/i...</td>\n","      <td>welcome r depression check post place take mom...</td>\n","      <td>regular check post plus reminder activism rule</td>\n","      <td>sql witch</td>\n","      <td>1225</td>\n","      <td>67</td>\n","      <td>sql witch welcome r depression check post plac...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I’ve wasted so much time being depressed, that...</td>\n","      <td>I can’t imagine how many days I have wasted to...</td>\n","      <td>RosiePosie710</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>https://www.reddit.com/r/depression/comments/j...</td>\n","      <td>imagine many day wasted sad cry able accomplis...</td>\n","      <td>wasted much time depressed depressed wasting time</td>\n","      <td>rosie po ie 710</td>\n","      <td>323</td>\n","      <td>80</td>\n","      <td>rosie po ie 710 imagine many day wasted sad cr...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               title  \\\n","0  Our most-broken and least-understood rules is ...   \n","1  Regular Check-In Post. Plus, a reminder about ...   \n","2  I’ve wasted so much time being depressed, that...   \n","\n","                                            selftext         author  \\\n","0  We understand that most people who reply immed...       SQLwitch   \n","1  Welcome to /r/depression's check-in post - a p...       SQLwitch   \n","2  I can’t imagine how many days I have wasted to...  RosiePosie710   \n","\n","   num_comments  is_suicide  \\\n","0           175           0   \n","1          1237           0   \n","2           104           0   \n","\n","                                                 url  \\\n","0  https://www.reddit.com/r/depression/comments/d...   \n","1  https://www.reddit.com/r/depression/comments/i...   \n","2  https://www.reddit.com/r/depression/comments/j...   \n","\n","                                      selftext_clean  \\\n","0  understand people reply immediately op invitat...   \n","1  welcome r depression check post place take mom...   \n","2  imagine many day wasted sad cry able accomplis...   \n","\n","                                         title_clean     author_clean  \\\n","0  broken least understood rule helper may invite...        sql witch   \n","1     regular check post plus reminder activism rule        sql witch   \n","2  wasted much time depressed depressed wasting time  rosie po ie 710   \n","\n","   selftext_length  title_length  \\\n","0             4792           144   \n","1             1225            67   \n","2              323            80   \n","\n","                                      megatext_clean  \n","0  sql witch understand people reply immediately ...  \n","1  sql witch welcome r depression check post plac...  \n","2  rosie po ie 710 imagine many day wasted sad cr...  "]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"2r_sBPSBsuj5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605063697542,"user_tz":-330,"elapsed":1673,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"d0c84413-2e1e-4585-e189-28a0b33cb579"},"source":["model_data.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1982 entries, 0 to 1981\n","Data columns (total 12 columns):\n"," #   Column           Non-Null Count  Dtype \n","---  ------           --------------  ----- \n"," 0   title            1982 non-null   object\n"," 1   selftext         1982 non-null   object\n"," 2   author           1982 non-null   object\n"," 3   num_comments     1982 non-null   int64 \n"," 4   is_suicide       1982 non-null   int64 \n"," 5   url              1982 non-null   object\n"," 6   selftext_clean   1982 non-null   object\n"," 7   title_clean      1982 non-null   object\n"," 8   author_clean     1982 non-null   object\n"," 9   selftext_length  1982 non-null   int64 \n"," 10  title_length     1982 non-null   int64 \n"," 11  megatext_clean   1982 non-null   object\n","dtypes: int64(4), object(8)\n","memory usage: 185.9+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HV0iKK_-suj-"},"source":["### 3.1 Establishing a baseline score \n","We will first calculate the baseline score for our models to \"out-perform\". A baseline score in the context of our project be the percentage of us getting it right if we predict that all our reddit posts are from the r/SuicideWatch subreddit. \n"]},{"cell_type":"code","metadata":{"id":"DQM98cIdsuj_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605063707470,"user_tz":-330,"elapsed":2178,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"f500edde-3626-4abd-8d2e-2f5700772b88"},"source":["#OUR BASELINE ACCURACY IS 51.66%\n","#BASELINE ACCURACY IS BASICALLY OUR SCORE IF WE GUESS EVERYTHING == 1\n","model_data['is_suicide'].mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49848637739656915"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"0OguxCwSsukE"},"source":["### 3.2 Selecting the best column to pick our features from\n","Before moving forward to creating a production model, we will run a Count Vectorizer + Naive Bayes model on different columns and score them. This will help us pick which one that we will use to build more models on. "]},{"cell_type":"code","metadata":{"id":"WovtlcaXsukF"},"source":["#DEFINING A FUNCTION TO SCORE MULTIPLE MODELS USING DIFFERENT COLS OF OUR DATASET\n","\n","#PLEASE DEFINE THE FOLLOWING BEFORE RUNNING THE FUNCTION\n","# columns_list = ['column_1', \"column_2\", \"column_3\"]\n","# model = \"CountVec + MultinomialNB\"\n","# df_list=[] #THIS LIST SHOULD BE AN EMPTY LIST\n","\n","#DEFINING THE FUNCTION\n","def multi_modelling(columns_list, model):\n","    for i in columns_list:\n","        #DEFINING X and y\n","        X = model_data[i]\n","        y = model_data['is_suicide']\n","        \n","        #TRAIN-TEST SPLIT\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n","        \n","        #INSTATIATING CVEC AND FITTING OUR TRAINING DATA INTO IT\n","        cvec = CountVectorizer()\n","        cvec.fit(X_train)\n","        \n","        #CREATING DATAFRAMES FROM X_train AND X_test\n","        X_train = pd.DataFrame(cvec.transform(X_train).todense(),\n","                               columns=cvec.get_feature_names())\n","        X_test = pd.DataFrame(cvec.transform(X_test).todense(),\n","                               columns=cvec.get_feature_names())\n","        \n","        #INSTATIATING AND FITTING MODEL\n","        nb = MultinomialNB()\n","        nb.fit(X_train,y_train)\n","        \n","        #GETTING PREDICTIONS FROM MODEL\n","        pred = nb.predict(X_test)\n","        \n","        #GETTING VALUES FROM A CONFUSION MATRIX\n","        tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n","        \n","        \n","        #CALCULATING AREA UNDER THE CURVE\n","        nb.predict_proba(X_test)\n","        pred_proba = [i[1] for i in nb.predict_proba(X_test)] \n","        auc = roc_auc_score(y_test, pred_proba)\n","\n","        #CREATING A DICTIONARY FROM THE CLASSIFICATION REPORT(WE'LL DRAW SOME METRICS FROM HERE)\n","        classi_dict = (classification_report(y_test,pred, output_dict=True))\n","\n","        #CREATING A DICTIONARY CONTAINING OUR RESULTS\n","        model_results = {}\n","        model_results['series used (X)'] = i\n","        model_results['model'] = model\n","        model_results['AUC Score'] = auc\n","        model_results['precision']= classi_dict['weighted avg']['precision']\n","        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n","        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n","        model_results['train accuracy'] = nb.score(X_train, y_train)\n","        model_results['test accuracy'] = nb.score(X_test, y_test)\n","        model_results['baseline accuracy']=0.5166\n","        model_results['specificity']= tn/(tn+fp)  \n","        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n","        #model_results['support']= classi_dict['weighted avg']['support']\n","        model_results\n","        df_list.append(model_results) \n","\n","    pd.set_option(\"display.max_colwidth\", 50)\n","    return (pd.DataFrame(df_list)).round(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MX-_-vYlsukJ"},"source":["#### Note: Understanding our confusion matrix\n","In the context of our project, these are what the parameters in our confusion matrix represent: \n","\n","> **True Positives (TP)** -  We predict that an entry is from the r/SuicideWatch subreddit and we get it right. As we are seeking to identify suicide cases, our priority is to get as many of these!\n","\n","> **True Negatives (TN)** - We predict that an entry is from the r/depression subreddit and we get it right. This also means that we did well. \n","\n","> **False Positives (FP)** - We predict that an entry is from the r/SuicideWatch subreddit and we get it wrong. Needless to say, this is undesirable. \n","\n","> **False Negatives (FN)** - We predict that an entry is from the r/depression subreddit and BUT the entry is actually from r/SuicideWatch. This is the worst outcome. That means we might be missing out on helping someone who might be thinking about ending their life.  "]},{"cell_type":"code","metadata":{"id":"0bqJO0PpsukJ","colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"status":"ok","timestamp":1605063809943,"user_tz":-330,"elapsed":4495,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"d2a0c3c1-c065-472a-b62d-9ecfb9af1b2e"},"source":["#CALLING THE FUNCTION ON OUR LIST OF COLUMNS\n","columns_list = ['selftext', \"author\", \"title\",'selftext_clean', \"author_clean\", \"title_clean\", \"megatext_clean\"]\n","model = \"CountVec + MultinomialNB\"\n","df_list=[]\n","multi_modelling(columns_list, model)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>series used (X)</th>\n","      <th>model</th>\n","      <th>AUC Score</th>\n","      <th>precision</th>\n","      <th>recall (sensitivity)</th>\n","      <th>confusion matrix</th>\n","      <th>train accuracy</th>\n","      <th>test accuracy</th>\n","      <th>baseline accuracy</th>\n","      <th>specificity</th>\n","      <th>f1-score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>selftext</td>\n","      <td>CountVec + MultinomialNB</td>\n","      <td>0.71</td>\n","      <td>0.68</td>\n","      <td>0.68</td>\n","      <td>{'TP': 183, 'FP': 96, 'TN': 153, 'FN': 64}</td>\n","      <td>0.88</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.61</td>\n","      <td>0.68</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>author</td>\n","      <td>CountVec + MultinomialNB</td>\n","      <td>0.60</td>\n","      <td>0.70</td>\n","      <td>0.58</td>\n","      <td>{'TP': 47, 'FP': 8, 'TN': 241, 'FN': 200}</td>\n","      <td>0.98</td>\n","      <td>0.58</td>\n","      <td>0.52</td>\n","      <td>0.97</td>\n","      <td>0.51</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>title</td>\n","      <td>CountVec + MultinomialNB</td>\n","      <td>0.69</td>\n","      <td>0.66</td>\n","      <td>0.66</td>\n","      <td>{'TP': 172, 'FP': 94, 'TN': 155, 'FN': 75}</td>\n","      <td>0.83</td>\n","      <td>0.66</td>\n","      <td>0.52</td>\n","      <td>0.62</td>\n","      <td>0.66</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>selftext_clean</td>\n","      <td>CountVec + MultinomialNB</td>\n","      <td>0.71</td>\n","      <td>0.68</td>\n","      <td>0.68</td>\n","      <td>{'TP': 181, 'FP': 95, 'TN': 154, 'FN': 66}</td>\n","      <td>0.89</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.62</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>author_clean</td>\n","      <td>CountVec + MultinomialNB</td>\n","      <td>0.59</td>\n","      <td>0.57</td>\n","      <td>0.57</td>\n","      <td>{'TP': 120, 'FP': 86, 'TN': 163, 'FN': 127}</td>\n","      <td>0.94</td>\n","      <td>0.57</td>\n","      <td>0.52</td>\n","      <td>0.65</td>\n","      <td>0.57</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>title_clean</td>\n","      <td>CountVec + MultinomialNB</td>\n","      <td>0.69</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'TP': 165, 'FP': 92, 'TN': 157, 'FN': 82}</td>\n","      <td>0.82</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.63</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>megatext_clean</td>\n","      <td>CountVec + MultinomialNB</td>\n","      <td>0.76</td>\n","      <td>0.71</td>\n","      <td>0.71</td>\n","      <td>{'TP': 187, 'FP': 86, 'TN': 163, 'FN': 60}</td>\n","      <td>0.93</td>\n","      <td>0.71</td>\n","      <td>0.52</td>\n","      <td>0.65</td>\n","      <td>0.70</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  series used (X)                     model  AUC Score  precision  \\\n","0        selftext  CountVec + MultinomialNB       0.71       0.68   \n","1          author  CountVec + MultinomialNB       0.60       0.70   \n","2           title  CountVec + MultinomialNB       0.69       0.66   \n","3  selftext_clean  CountVec + MultinomialNB       0.71       0.68   \n","4    author_clean  CountVec + MultinomialNB       0.59       0.57   \n","5     title_clean  CountVec + MultinomialNB       0.69       0.65   \n","6  megatext_clean  CountVec + MultinomialNB       0.76       0.71   \n","\n","   recall (sensitivity)                             confusion matrix  \\\n","0                  0.68   {'TP': 183, 'FP': 96, 'TN': 153, 'FN': 64}   \n","1                  0.58    {'TP': 47, 'FP': 8, 'TN': 241, 'FN': 200}   \n","2                  0.66   {'TP': 172, 'FP': 94, 'TN': 155, 'FN': 75}   \n","3                  0.68   {'TP': 181, 'FP': 95, 'TN': 154, 'FN': 66}   \n","4                  0.57  {'TP': 120, 'FP': 86, 'TN': 163, 'FN': 127}   \n","5                  0.65   {'TP': 165, 'FP': 92, 'TN': 157, 'FN': 82}   \n","6                  0.71   {'TP': 187, 'FP': 86, 'TN': 163, 'FN': 60}   \n","\n","   train accuracy  test accuracy  baseline accuracy  specificity  f1-score  \n","0            0.88           0.68               0.52         0.61      0.68  \n","1            0.98           0.58               0.52         0.97      0.51  \n","2            0.83           0.66               0.52         0.62      0.66  \n","3            0.89           0.68               0.52         0.62      0.67  \n","4            0.94           0.57               0.52         0.65      0.57  \n","5            0.82           0.65               0.52         0.63      0.65  \n","6            0.93           0.71               0.52         0.65      0.70  "]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"sqsEg5TZsukO"},"source":["#### Final choice made: *megatext_clean* as our \"Production Column\"\n","Based on a combination of scores from our modelling exercise above, we will proceed with *megatext_clean* -- a combination of our cleaned titles, usernames and posts --  as the column we will use to draw features from. Some reasons why: \n","\n","> **Generalising Well** -  The model using *megatext_clean*'s test set scored a 0.67 (the joint highest) while its training set score a 0.95.\n","\n","> **High ROC Area Under Curve score** - As our classes are largely balanced, it is suitable to use AUC Scores as a metric to measure the quality of our model's predictions. Our top choice performs best there. \n","\n","> **Best recall/sensitivity score** - This score measures the ratio of the correctly positive-labeled(is in r/SuicideWatch) by our program to all who are truly in r/SuicideWatch. As that is the target of our project, that the model performed well for this metric is important(and perhaps, most important) to us.\n","\n","\n","\n","> **False Negatives (FN)** - We predict that an entry is from the r/depression subreddit and BUT the entry is actually from r/SuicideWatch. This is the worst outcome. That means we might be missing out on helping someone who might be thinking about ending their life.  "]},{"cell_type":"markdown","metadata":{"id":"50eyU3gZsukO"},"source":["### 3.3 The search for a production model\n","Inspired by our earlier function, we will create a similar function that will run multiple permutations of models with Count, Hashing and TFID Vectorizers. The resulting metrics will be held neatly in a dataframe. "]},{"cell_type":"code","metadata":{"id":"-Dgn0nftsukP"},"source":["# DEFINING A FUNCTION THAT WILL RUN MULTIPLE MODELS AND GRIDSEARCH FOR BEST PARAMETERS\n","\n","def gridsearch_multi(steps_titles, steps_list, pipe_params):\n","    \n","    #DEFINING X and y\n","    X = model_data[\"megatext_clean\"]\n","    y = model_data['is_suicide']\n","    #TRAIN-TEST SPLIT\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n","    # DATAFRAME TO DISPLAY RESULTS\n","    gs_results = pd.DataFrame(columns=['model','AUC Score', 'precision', 'recall (sensitivity)', \n","                                       'best_params', 'best score', 'confusion matrix', \n","                                       'train_accuracy','test_accuracy','baseline_accuracy',\n","                                       'specificity', 'f1-score'])\n","\n","    # FOR LOOP THROUGH STEPS LIST\n","    for i in range(len(steps_list)):\n","        # INSTATIATE PIPELINE\n","        pipe = Pipeline(steps=steps_list[i])\n","        # INSTANTIATE GRIDSEARCHCV WITH PARAMETER ARGUMENT\n","        gs = GridSearchCV(pipe, pipe_params[i], cv=3) \n","        gs.fit(X_train, y_train)\n","        \n","        #GETTING PREDICTIONS FROM MODEL\n","        pred = gs.predict(X_test)\n","        \n","        # DEFINE CONFUSION MATRIX ELEMENTS\n","        tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n","        \n","        #CREATING A DICTIONARY FROM THE CLASSIFICATION REPORT(WE'LL DRAW SOME METRICS FROM HERE)\n","        classi_dict = (classification_report(y_test,pred, output_dict=True))\n","        \n","        #CALCULATING AREA UNDER THE CURVE\n","        gs.predict_proba(X_test)\n","        pred_proba = [i[1] for i in gs.predict_proba(X_test)] \n","        auc = roc_auc_score(y_test, pred_proba)\n","        \n","        #DEFINE DATAFRAME COLUMNS\n","        model_results = {}\n","        model_results['model'] = steps_titles[i]\n","        model_results['AUC Score'] = auc\n","        model_results['precision']= classi_dict['weighted avg']['precision']\n","        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n","        model_results['best params'] = gs.best_params_\n","        model_results['best score'] = gs.best_score_\n","        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n","        model_results['train accuracy'] = gs.score(X_train, y_train)\n","        model_results['test accuracy'] = gs.score(X_test, y_test)\n","        model_results['baseline accuracy'] = 0.5166\n","        \n","        model_results['specificity']= tn/(tn+fp)  \n","        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n","\n","\n","        #APPEND RESULTS TO A NICE DATAFRAME\n","        df_list.append(model_results) \n","        pd.set_option(\"display.max_colwidth\", 200)\n","    return (pd.DataFrame(df_list)).round(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"XYLaWBXtsukV","colab":{"base_uri":"https://localhost:8080/","height":304},"executionInfo":{"status":"ok","timestamp":1605064214210,"user_tz":-330,"elapsed":72888,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"f7b87277-e7f3-4fb7-94ca-46b80997daec"},"source":["#USING THE FUNCTION WITH COUNT VECTORIZOR\n","\n","# EMPTY LIST THAT WILL HOLD RESULTS\n","df_list=[]\n","\n","# LIST OF MODELS\n","steps_titles = ['cvec+ multi_nb','cvec + ss + knn','cvec + ss + logreg']\n","\n","# CODE FOR PIPELINE TO INSTATIATE MODELS\n","steps_list = [ \n","    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n","    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n","    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n","]\n","\n","# LIST OF PARAMETER DICTIONARIES\n","pipe_params = [\n","    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n","    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n","    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]}\n","]   \n","\n","#RUNNING THE FUNCTION\n","gridsearch_multi(steps_titles, steps_list, pipe_params)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>AUC Score</th>\n","      <th>precision</th>\n","      <th>recall (sensitivity)</th>\n","      <th>best params</th>\n","      <th>best score</th>\n","      <th>confusion matrix</th>\n","      <th>train accuracy</th>\n","      <th>test accuracy</th>\n","      <th>baseline accuracy</th>\n","      <th>specificity</th>\n","      <th>f1-score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cvec+ multi_nb</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cvec + ss + knn</td>\n","      <td>0.66</td>\n","      <td>0.62</td>\n","      <td>0.62</td>\n","      <td>{'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}</td>\n","      <td>0.73</td>\n","      <td>0.62</td>\n","      <td>0.52</td>\n","      <td>0.65</td>\n","      <td>0.62</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cvec + ss + logreg</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}</td>\n","      <td>0.72</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                model  AUC Score  precision  recall (sensitivity)  \\\n","0      cvec+ multi_nb       0.73       0.67                  0.67   \n","1     cvec + ss + knn       0.66       0.62                  0.62   \n","2  cvec + ss + logreg       0.73       0.67                  0.67   \n","\n","                                                                                                             best params  \\\n","0   {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","1  {'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","\n","   best score                             confusion matrix  train accuracy  \\\n","0        0.69   {'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}            0.71   \n","1        0.61  {'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}            0.73   \n","2        0.68   {'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}            0.72   \n","\n","   test accuracy  baseline accuracy  specificity  f1-score  \n","0           0.67               0.52         0.67      0.67  \n","1           0.62               0.52         0.65      0.62  \n","2           0.67               0.52         0.71      0.67  "]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"zkTHZl-jsukZ","colab":{"base_uri":"https://localhost:8080/","height":544},"executionInfo":{"status":"ok","timestamp":1605065001186,"user_tz":-330,"elapsed":72339,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"b351db51-b1d7-45af-f3eb-206f6a9504bc"},"source":["#USING THE FUNCTION WITH TFID VECTORIZOR\n","\n","# LIST OF MODELS\n","steps_titles = ['tvec + multi_nb','tvec + ss + knn','tvec + ss + logreg']\n","\n","# CODE FOR PIPELINE TO INSTATIATE MODELS\n","steps_list = [ \n","    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())],\n","    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n","    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n","]\n","\n","# LIST OF PARAMETER DICTIONARIES\n","pipe_params = [\n","    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n","    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n","    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]}\n","]   \n","\n","#RUNNING THE FUNCTION\n","gridsearch_multi(steps_titles, steps_list, pipe_params)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>AUC Score</th>\n","      <th>precision</th>\n","      <th>recall (sensitivity)</th>\n","      <th>best params</th>\n","      <th>best score</th>\n","      <th>confusion matrix</th>\n","      <th>train accuracy</th>\n","      <th>test accuracy</th>\n","      <th>baseline accuracy</th>\n","      <th>specificity</th>\n","      <th>f1-score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cvec+ multi_nb</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cvec + ss + knn</td>\n","      <td>0.66</td>\n","      <td>0.62</td>\n","      <td>0.62</td>\n","      <td>{'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}</td>\n","      <td>0.73</td>\n","      <td>0.62</td>\n","      <td>0.52</td>\n","      <td>0.65</td>\n","      <td>0.62</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cvec + ss + logreg</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}</td>\n","      <td>0.72</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tvec + multi_nb</td>\n","      <td>0.74</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 146, 'FP': 72, 'TN': 177, 'FN': 101}</td>\n","      <td>0.71</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tvec + ss + knn</td>\n","      <td>0.63</td>\n","      <td>0.60</td>\n","      <td>0.60</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 20, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 137, 'FP': 89, 'TN': 160, 'FN': 110}</td>\n","      <td>0.71</td>\n","      <td>0.60</td>\n","      <td>0.52</td>\n","      <td>0.64</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>tvec + ss + logreg</td>\n","      <td>0.74</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 154, 'FP': 81, 'TN': 168, 'FN': 93}</td>\n","      <td>0.72</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.65</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                model  AUC Score  precision  recall (sensitivity)  \\\n","0      cvec+ multi_nb       0.73       0.67                  0.67   \n","1     cvec + ss + knn       0.66       0.62                  0.62   \n","2  cvec + ss + logreg       0.73       0.67                  0.67   \n","3     tvec + multi_nb       0.74       0.65                  0.65   \n","4     tvec + ss + knn       0.63       0.60                  0.60   \n","5  tvec + ss + logreg       0.74       0.65                  0.65   \n","\n","                                                                                                             best params  \\\n","0   {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","1  {'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","3   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","4   {'tv__max_df': 0.2, 'tv__max_features': 20, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","5   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","\n","   best score                             confusion matrix  train accuracy  \\\n","0        0.69   {'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}            0.71   \n","1        0.61  {'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}            0.73   \n","2        0.68   {'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}            0.72   \n","3        0.69  {'TP': 146, 'FP': 72, 'TN': 177, 'FN': 101}            0.71   \n","4        0.61  {'TP': 137, 'FP': 89, 'TN': 160, 'FN': 110}            0.71   \n","5        0.68   {'TP': 154, 'FP': 81, 'TN': 168, 'FN': 93}            0.72   \n","\n","   test accuracy  baseline accuracy  specificity  f1-score  \n","0           0.67               0.52         0.67      0.67  \n","1           0.62               0.52         0.65      0.62  \n","2           0.67               0.52         0.71      0.67  \n","3           0.65               0.52         0.71      0.65  \n","4           0.60               0.52         0.64      0.60  \n","5           0.65               0.52         0.67      0.65  "]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"Q2kbcSVZsukc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1605065112615,"user_tz":-330,"elapsed":111399,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"f3105e72-51da-4b87-87e4-bbaad3c99f3b"},"source":["#USING THE FUNCTION WITH HASHING VECTORIZOR\n","\n","# LIST OF MODELS\n","steps_titles = ['hvec + multi_nb','hvec + ss + knn','hvec + ss + logreg']\n","\n","# CODE FOR PIPELINE TO INSTATIATE MODELS\n","steps_list = [ \n","    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n","    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n","    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n","]\n","\n","# LIST OF PARAMETER DICTIONARIES\n","pipe_params = [\n","    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n","    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n","    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]}\n","]   \n","\n","#RUNNING THE FUNCTION\n","gridsearch_multi(steps_titles, steps_list, pipe_params)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>AUC Score</th>\n","      <th>precision</th>\n","      <th>recall (sensitivity)</th>\n","      <th>best params</th>\n","      <th>best score</th>\n","      <th>confusion matrix</th>\n","      <th>train accuracy</th>\n","      <th>test accuracy</th>\n","      <th>baseline accuracy</th>\n","      <th>specificity</th>\n","      <th>f1-score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cvec+ multi_nb</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cvec + ss + knn</td>\n","      <td>0.66</td>\n","      <td>0.62</td>\n","      <td>0.62</td>\n","      <td>{'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}</td>\n","      <td>0.73</td>\n","      <td>0.62</td>\n","      <td>0.52</td>\n","      <td>0.65</td>\n","      <td>0.62</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cvec + ss + logreg</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}</td>\n","      <td>0.72</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tvec + multi_nb</td>\n","      <td>0.74</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 146, 'FP': 72, 'TN': 177, 'FN': 101}</td>\n","      <td>0.71</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tvec + ss + knn</td>\n","      <td>0.63</td>\n","      <td>0.60</td>\n","      <td>0.60</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 20, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 137, 'FP': 89, 'TN': 160, 'FN': 110}</td>\n","      <td>0.71</td>\n","      <td>0.60</td>\n","      <td>0.52</td>\n","      <td>0.64</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>tvec + ss + logreg</td>\n","      <td>0.74</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 154, 'FP': 81, 'TN': 168, 'FN': 93}</td>\n","      <td>0.72</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>hvec + multi_nb</td>\n","      <td>0.76</td>\n","      <td>0.69</td>\n","      <td>0.68</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 136, 'FP': 50, 'TN': 199, 'FN': 111}</td>\n","      <td>0.95</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.80</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>hvec + ss + knn</td>\n","      <td>0.49</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.51</td>\n","      <td>{'TP': 247, 'FP': 247, 'TN': 2, 'FN': 0}</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.52</td>\n","      <td>0.01</td>\n","      <td>0.34</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>hvec + ss + logreg</td>\n","      <td>0.71</td>\n","      <td>0.64</td>\n","      <td>0.64</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.67</td>\n","      <td>{'TP': 145, 'FP': 78, 'TN': 171, 'FN': 102}</td>\n","      <td>1.00</td>\n","      <td>0.64</td>\n","      <td>0.52</td>\n","      <td>0.69</td>\n","      <td>0.64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                model  AUC Score  precision  recall (sensitivity)  \\\n","0      cvec+ multi_nb       0.73       0.67                  0.67   \n","1     cvec + ss + knn       0.66       0.62                  0.62   \n","2  cvec + ss + logreg       0.73       0.67                  0.67   \n","3     tvec + multi_nb       0.74       0.65                  0.65   \n","4     tvec + ss + knn       0.63       0.60                  0.60   \n","5  tvec + ss + logreg       0.74       0.65                  0.65   \n","6     hvec + multi_nb       0.76       0.69                  0.68   \n","7     hvec + ss + knn       0.49       0.75                  0.50   \n","8  hvec + ss + logreg       0.71       0.64                  0.64   \n","\n","                                                                                                             best params  \\\n","0   {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","1  {'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","3   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","4   {'tv__max_df': 0.2, 'tv__max_features': 20, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","5   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","6                                                               {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","7                                                               {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","8                                                               {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","\n","   best score                             confusion matrix  train accuracy  \\\n","0        0.69   {'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}            0.71   \n","1        0.61  {'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}            0.73   \n","2        0.68   {'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}            0.72   \n","3        0.69  {'TP': 146, 'FP': 72, 'TN': 177, 'FN': 101}            0.71   \n","4        0.61  {'TP': 137, 'FP': 89, 'TN': 160, 'FN': 110}            0.71   \n","5        0.68   {'TP': 154, 'FP': 81, 'TN': 168, 'FN': 93}            0.72   \n","6        0.68  {'TP': 136, 'FP': 50, 'TN': 199, 'FN': 111}            0.95   \n","7        0.51     {'TP': 247, 'FP': 247, 'TN': 2, 'FN': 0}            0.50   \n","8        0.67  {'TP': 145, 'FP': 78, 'TN': 171, 'FN': 102}            1.00   \n","\n","   test accuracy  baseline accuracy  specificity  f1-score  \n","0           0.67               0.52         0.67      0.67  \n","1           0.62               0.52         0.65      0.62  \n","2           0.67               0.52         0.71      0.67  \n","3           0.65               0.52         0.71      0.65  \n","4           0.60               0.52         0.64      0.60  \n","5           0.65               0.52         0.67      0.65  \n","6           0.68               0.52         0.80      0.67  \n","7           0.50               0.52         0.01      0.34  \n","8           0.64               0.52         0.69      0.64  "]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"aWidjJiZsukg"},"source":["#### Narrowing down to two models \n","The *Hashing Vectorizer + Multinomial Naive Bayes* model out-performed other models on multiple metrics. Especially our much-prized AUC score(0.77) and the recall score(which measures our model's ability to predict True Positives well). Another notable performer is the *TFID Vectorizer + Multinomial Naive Bayes* combination. Apart from the joint-second-highest AUC score of 0.73, its consistent performance on both the test and training sets showed that the model generalises well.\n","\n","> **Next Step: Tuning Hyperparameters** -  We'll now move on to make further moves to tweak our hyperparameters for both of these models. "]},{"cell_type":"code","metadata":{"id":"QYtvmv5Vsuki","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1605065271471,"user_tz":-330,"elapsed":270248,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"79048bdf-7abd-42e0-c738-4536fa9c1a27"},"source":["#TUNING OUR HYPERPARAMETERS\n","#USING THE FUNCTION ON OUR TOP TWO MODELS\n","\n","# LIST OF MODELS\n","steps_titles = ['hvec + multi_nb(tuning)','tvec + multi_nb(tuning)']\n","\n","# CODE FOR PIPELINE TO INSTATIATE MODELS\n","steps_list = [ \n","    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n","    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())]\n","]\n","\n","# LIST OF PARAMETER DICTIONARIES\n","pipe_params = [\n","    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2),(1,3)], 'hv__n_features': [50, 150, 300, 500, 800, 1000]},\n","    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2),(1,3)],'tv__max_features': [20, 30, 50, 70, 100],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3, .35,.4]},\n","]   \n","\n","#RUNNING THE FUNCTION\n","gridsearch_multi(steps_titles, steps_list, pipe_params)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>AUC Score</th>\n","      <th>precision</th>\n","      <th>recall (sensitivity)</th>\n","      <th>best params</th>\n","      <th>best score</th>\n","      <th>confusion matrix</th>\n","      <th>train accuracy</th>\n","      <th>test accuracy</th>\n","      <th>baseline accuracy</th>\n","      <th>specificity</th>\n","      <th>f1-score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cvec+ multi_nb</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cvec + ss + knn</td>\n","      <td>0.66</td>\n","      <td>0.62</td>\n","      <td>0.62</td>\n","      <td>{'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}</td>\n","      <td>0.73</td>\n","      <td>0.62</td>\n","      <td>0.52</td>\n","      <td>0.65</td>\n","      <td>0.62</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cvec + ss + logreg</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}</td>\n","      <td>0.72</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tvec + multi_nb</td>\n","      <td>0.74</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 146, 'FP': 72, 'TN': 177, 'FN': 101}</td>\n","      <td>0.71</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tvec + ss + knn</td>\n","      <td>0.63</td>\n","      <td>0.60</td>\n","      <td>0.60</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 20, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 137, 'FP': 89, 'TN': 160, 'FN': 110}</td>\n","      <td>0.71</td>\n","      <td>0.60</td>\n","      <td>0.52</td>\n","      <td>0.64</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>tvec + ss + logreg</td>\n","      <td>0.74</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 154, 'FP': 81, 'TN': 168, 'FN': 93}</td>\n","      <td>0.72</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>hvec + multi_nb</td>\n","      <td>0.76</td>\n","      <td>0.69</td>\n","      <td>0.68</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 136, 'FP': 50, 'TN': 199, 'FN': 111}</td>\n","      <td>0.95</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.80</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>hvec + ss + knn</td>\n","      <td>0.49</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.51</td>\n","      <td>{'TP': 247, 'FP': 247, 'TN': 2, 'FN': 0}</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.52</td>\n","      <td>0.01</td>\n","      <td>0.34</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>hvec + ss + logreg</td>\n","      <td>0.71</td>\n","      <td>0.64</td>\n","      <td>0.64</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.67</td>\n","      <td>{'TP': 145, 'FP': 78, 'TN': 171, 'FN': 102}</td>\n","      <td>1.00</td>\n","      <td>0.64</td>\n","      <td>0.52</td>\n","      <td>0.69</td>\n","      <td>0.64</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>hvec + multi_nb(tuning)</td>\n","      <td>0.75</td>\n","      <td>0.68</td>\n","      <td>0.68</td>\n","      <td>{'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 170, 'FP': 80, 'TN': 169, 'FN': 77}</td>\n","      <td>0.80</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.68</td>\n","      <td>0.68</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>tvec + multi_nb(tuning)</td>\n","      <td>0.75</td>\n","      <td>0.68</td>\n","      <td>0.67</td>\n","      <td>{'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n","      <td>0.70</td>\n","      <td>{'TP': 151, 'FP': 66, 'TN': 183, 'FN': 96}</td>\n","      <td>0.72</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      model  AUC Score  precision  recall (sensitivity)  \\\n","0            cvec+ multi_nb       0.73       0.67                  0.67   \n","1           cvec + ss + knn       0.66       0.62                  0.62   \n","2        cvec + ss + logreg       0.73       0.67                  0.67   \n","3           tvec + multi_nb       0.74       0.65                  0.65   \n","4           tvec + ss + knn       0.63       0.60                  0.60   \n","5        tvec + ss + logreg       0.74       0.65                  0.65   \n","6           hvec + multi_nb       0.76       0.69                  0.68   \n","7           hvec + ss + knn       0.49       0.75                  0.50   \n","8        hvec + ss + logreg       0.71       0.64                  0.64   \n","9   hvec + multi_nb(tuning)       0.75       0.68                  0.68   \n","10  tvec + multi_nb(tuning)       0.75       0.68                  0.67   \n","\n","                                                                                                              best params  \\\n","0    {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","1   {'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","2    {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","3    {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","4    {'tv__max_df': 0.2, 'tv__max_features': 20, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","5    {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","6                                                                {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","7                                                                {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","8                                                                {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","9                                        {'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n","10   {'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n","\n","    best score                             confusion matrix  train accuracy  \\\n","0         0.69   {'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}            0.71   \n","1         0.61  {'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}            0.73   \n","2         0.68   {'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}            0.72   \n","3         0.69  {'TP': 146, 'FP': 72, 'TN': 177, 'FN': 101}            0.71   \n","4         0.61  {'TP': 137, 'FP': 89, 'TN': 160, 'FN': 110}            0.71   \n","5         0.68   {'TP': 154, 'FP': 81, 'TN': 168, 'FN': 93}            0.72   \n","6         0.68  {'TP': 136, 'FP': 50, 'TN': 199, 'FN': 111}            0.95   \n","7         0.51     {'TP': 247, 'FP': 247, 'TN': 2, 'FN': 0}            0.50   \n","8         0.67  {'TP': 145, 'FP': 78, 'TN': 171, 'FN': 102}            1.00   \n","9         0.69   {'TP': 170, 'FP': 80, 'TN': 169, 'FN': 77}            0.80   \n","10        0.70   {'TP': 151, 'FP': 66, 'TN': 183, 'FN': 96}            0.72   \n","\n","    test accuracy  baseline accuracy  specificity  f1-score  \n","0            0.67               0.52         0.67      0.67  \n","1            0.62               0.52         0.65      0.62  \n","2            0.67               0.52         0.71      0.67  \n","3            0.65               0.52         0.71      0.65  \n","4            0.60               0.52         0.64      0.60  \n","5            0.65               0.52         0.67      0.65  \n","6            0.68               0.52         0.80      0.67  \n","7            0.50               0.52         0.01      0.34  \n","8            0.64               0.52         0.69      0.64  \n","9            0.68               0.52         0.68      0.68  \n","10           0.67               0.52         0.73      0.67  "]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"5H5lBh1-suko","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1605065501065,"user_tz":-330,"elapsed":499837,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"62a3120d-a1e9-4269-e15d-e4f4fb8ca9c6"},"source":["#THE SCORES WERE CLOSE! LET'S DO FINE-TUNE IT JUST ONE MORE TIME\n","#TUNING OUR HYPERPARAMETERS PART II\n","#USING THE FUNCTION ON OUR TOP TWO MODELS\n","\n","# LIST OF MODELS\n","steps_titles = ['hvec + multi_nb (tuning_2)','tvec + multi_nb (tuning_2)']\n","\n","# CODE FOR PIPELINE TO INSTATIATE MODELS\n","steps_list = [ \n","    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n","    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())]\n","]\n","\n","# LIST OF PARAMETER DICTIONARIES\n","pipe_params = [\n","    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1)], 'hv__n_features': [1000, 1200, 1400, 2000]},\n","    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2),(1,3)],'tv__max_features': [60, 65, 70, 75, 80],'tv__min_df': [1, 2, 3],'tv__max_df': [.4, .45,.5,.55, .6]},\n","]   \n","\n","#RUNNING THE FUNCTION\n","gridsearch_multi(steps_titles, steps_list, pipe_params)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>AUC Score</th>\n","      <th>precision</th>\n","      <th>recall (sensitivity)</th>\n","      <th>best params</th>\n","      <th>best score</th>\n","      <th>confusion matrix</th>\n","      <th>train accuracy</th>\n","      <th>test accuracy</th>\n","      <th>baseline accuracy</th>\n","      <th>specificity</th>\n","      <th>f1-score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cvec+ multi_nb</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cvec + ss + knn</td>\n","      <td>0.66</td>\n","      <td>0.62</td>\n","      <td>0.62</td>\n","      <td>{'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}</td>\n","      <td>0.73</td>\n","      <td>0.62</td>\n","      <td>0.52</td>\n","      <td>0.65</td>\n","      <td>0.62</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cvec + ss + logreg</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","      <td>0.67</td>\n","      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}</td>\n","      <td>0.72</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tvec + multi_nb</td>\n","      <td>0.74</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 146, 'FP': 72, 'TN': 177, 'FN': 101}</td>\n","      <td>0.71</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tvec + ss + knn</td>\n","      <td>0.63</td>\n","      <td>0.60</td>\n","      <td>0.60</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 20, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.61</td>\n","      <td>{'TP': 137, 'FP': 89, 'TN': 160, 'FN': 110}</td>\n","      <td>0.71</td>\n","      <td>0.60</td>\n","      <td>0.52</td>\n","      <td>0.64</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>tvec + ss + logreg</td>\n","      <td>0.74</td>\n","      <td>0.65</td>\n","      <td>0.65</td>\n","      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 154, 'FP': 81, 'TN': 168, 'FN': 93}</td>\n","      <td>0.72</td>\n","      <td>0.65</td>\n","      <td>0.52</td>\n","      <td>0.67</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>hvec + multi_nb</td>\n","      <td>0.76</td>\n","      <td>0.69</td>\n","      <td>0.68</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.68</td>\n","      <td>{'TP': 136, 'FP': 50, 'TN': 199, 'FN': 111}</td>\n","      <td>0.95</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.80</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>hvec + ss + knn</td>\n","      <td>0.49</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.51</td>\n","      <td>{'TP': 247, 'FP': 247, 'TN': 2, 'FN': 0}</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.52</td>\n","      <td>0.01</td>\n","      <td>0.34</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>hvec + ss + logreg</td>\n","      <td>0.71</td>\n","      <td>0.64</td>\n","      <td>0.64</td>\n","      <td>{'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}</td>\n","      <td>0.67</td>\n","      <td>{'TP': 145, 'FP': 78, 'TN': 171, 'FN': 102}</td>\n","      <td>1.00</td>\n","      <td>0.64</td>\n","      <td>0.52</td>\n","      <td>0.69</td>\n","      <td>0.64</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>hvec + multi_nb(tuning)</td>\n","      <td>0.75</td>\n","      <td>0.68</td>\n","      <td>0.68</td>\n","      <td>{'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n","      <td>0.69</td>\n","      <td>{'TP': 170, 'FP': 80, 'TN': 169, 'FN': 77}</td>\n","      <td>0.80</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.68</td>\n","      <td>0.68</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>tvec + multi_nb(tuning)</td>\n","      <td>0.75</td>\n","      <td>0.68</td>\n","      <td>0.67</td>\n","      <td>{'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n","      <td>0.70</td>\n","      <td>{'TP': 151, 'FP': 66, 'TN': 183, 'FN': 96}</td>\n","      <td>0.72</td>\n","      <td>0.67</td>\n","      <td>0.52</td>\n","      <td>0.73</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>hvec + multi_nb (tuning_2)</td>\n","      <td>0.76</td>\n","      <td>0.69</td>\n","      <td>0.69</td>\n","      <td>{'hv__n_features': 2000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n","      <td>0.70</td>\n","      <td>{'TP': 168, 'FP': 76, 'TN': 173, 'FN': 79}</td>\n","      <td>0.83</td>\n","      <td>0.69</td>\n","      <td>0.52</td>\n","      <td>0.69</td>\n","      <td>0.69</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>tvec + multi_nb (tuning_2)</td>\n","      <td>0.75</td>\n","      <td>0.68</td>\n","      <td>0.68</td>\n","      <td>{'tv__max_df': 0.5, 'tv__max_features': 80, 'tv__min_df': 1, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n","      <td>0.70</td>\n","      <td>{'TP': 159, 'FP': 73, 'TN': 176, 'FN': 88}</td>\n","      <td>0.72</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.71</td>\n","      <td>0.68</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         model  AUC Score  precision  recall (sensitivity)  \\\n","0               cvec+ multi_nb       0.73       0.67                  0.67   \n","1              cvec + ss + knn       0.66       0.62                  0.62   \n","2           cvec + ss + logreg       0.73       0.67                  0.67   \n","3              tvec + multi_nb       0.74       0.65                  0.65   \n","4              tvec + ss + knn       0.63       0.60                  0.60   \n","5           tvec + ss + logreg       0.74       0.65                  0.65   \n","6              hvec + multi_nb       0.76       0.69                  0.68   \n","7              hvec + ss + knn       0.49       0.75                  0.50   \n","8           hvec + ss + logreg       0.71       0.64                  0.64   \n","9      hvec + multi_nb(tuning)       0.75       0.68                  0.68   \n","10     tvec + multi_nb(tuning)       0.75       0.68                  0.67   \n","11  hvec + multi_nb (tuning_2)       0.76       0.69                  0.69   \n","12  tvec + multi_nb (tuning_2)       0.75       0.68                  0.68   \n","\n","                                                                                                              best params  \\\n","0    {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","1   {'cv__max_df': 0.25, 'cv__max_features': 30, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","2    {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n","3    {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","4    {'tv__max_df': 0.2, 'tv__max_features': 20, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","5    {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","6                                                                {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","7                                                                {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","8                                                                {'hv__ngram_range': (1, 2), 'hv__stop_words': 'english'}   \n","9                                        {'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n","10   {'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n","11                                       {'hv__n_features': 2000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n","12   {'tv__max_df': 0.5, 'tv__max_features': 80, 'tv__min_df': 1, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n","\n","    best score                             confusion matrix  train accuracy  \\\n","0         0.69   {'TP': 165, 'FP': 83, 'TN': 166, 'FN': 82}            0.71   \n","1         0.61  {'TP': 147, 'FP': 87, 'TN': 162, 'FN': 100}            0.73   \n","2         0.68   {'TP': 156, 'FP': 73, 'TN': 176, 'FN': 91}            0.72   \n","3         0.69  {'TP': 146, 'FP': 72, 'TN': 177, 'FN': 101}            0.71   \n","4         0.61  {'TP': 137, 'FP': 89, 'TN': 160, 'FN': 110}            0.71   \n","5         0.68   {'TP': 154, 'FP': 81, 'TN': 168, 'FN': 93}            0.72   \n","6         0.68  {'TP': 136, 'FP': 50, 'TN': 199, 'FN': 111}            0.95   \n","7         0.51     {'TP': 247, 'FP': 247, 'TN': 2, 'FN': 0}            0.50   \n","8         0.67  {'TP': 145, 'FP': 78, 'TN': 171, 'FN': 102}            1.00   \n","9         0.69   {'TP': 170, 'FP': 80, 'TN': 169, 'FN': 77}            0.80   \n","10        0.70   {'TP': 151, 'FP': 66, 'TN': 183, 'FN': 96}            0.72   \n","11        0.70   {'TP': 168, 'FP': 76, 'TN': 173, 'FN': 79}            0.83   \n","12        0.70   {'TP': 159, 'FP': 73, 'TN': 176, 'FN': 88}            0.72   \n","\n","    test accuracy  baseline accuracy  specificity  f1-score  \n","0            0.67               0.52         0.67      0.67  \n","1            0.62               0.52         0.65      0.62  \n","2            0.67               0.52         0.71      0.67  \n","3            0.65               0.52         0.71      0.65  \n","4            0.60               0.52         0.64      0.60  \n","5            0.65               0.52         0.67      0.65  \n","6            0.68               0.52         0.80      0.67  \n","7            0.50               0.52         0.01      0.34  \n","8            0.64               0.52         0.69      0.64  \n","9            0.68               0.52         0.68      0.68  \n","10           0.67               0.52         0.73      0.67  \n","11           0.69               0.52         0.69      0.69  \n","12           0.68               0.52         0.71      0.68  "]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"UOxwObTXsukt"},"source":["#### Production Model Chosen: TF-IDF Vectorizer + Multinomial Naive Bayes\n","The model responded well to the tuning sessions. Although the Hashing model had a slightly better AUC score, I'd prioritise this model's high recall score as it will help predict potential suicide cases(True Positives) more accurately. This model is also proving to generalise pretty well with only a 0.01 variation from its Training to Test set scores. "]},{"cell_type":"markdown","metadata":{"id":"B7g49hU4suku"},"source":["### 3.4 Running the optimised production model\n"]},{"cell_type":"markdown","metadata":{"id":"Ox_861s6suku"},"source":["Our production model is a combination of two models: TF-IDF and Multinomial Naive Bayes.\n","\n","The first one, a TF-IDF (or “Term Frequency – Inverse Document” Frequency) Vectorizer, assigns scores to the words (or in our case, the top 70 words) in our selected feature. TF-IDF will penalise a word that appears too often in the document. \n","\n","A matrix of \"word scores\" is then transferred into a Multinomial Naive Bayes classifier, which makes predictions based on the calculation of the probability of a given word falling into the a certain category.\n","\n"]},{"cell_type":"code","metadata":{"id":"inviDC8rsukv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605065501569,"user_tz":-330,"elapsed":500335,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"506d651e-644e-426a-af95-53ed248405e1"},"source":["#CHECKING SCORES OF THE OPTIMISED MODEL USING TEST DATA\n","#DEFINING X and y\n","X = model_data[\"megatext_clean\"]\n","y = model_data['is_suicide']\n","#TRAIN-TEST SPLIT\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n","\n","tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n","X_train_tvec = tvec_optimised.fit_transform(X_train).todense()\n","X_test_tvec = tvec_optimised.transform(X_test).todense()\n","\n","#FINDING THE ACCURACY SCORE ON THE TEST DATA\n","nb = MultinomialNB()\n","nb.fit(X_train_tvec, y_train)\n","accuracy = nb.score(X_test_tvec, y_test)\n","\n","#CALCULATING AREA UNDER THE CURVE\n","\n","pred_proba = [i[1] for i in nb.predict_proba(X_test_tvec)] \n","auc = roc_auc_score(y_test, pred_proba)\n","\n","print(\"ACCURACY: {}\\nAUC SCORE: {}\".format(accuracy, auc) )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ACCURACY: 0.6673387096774194\n","AUC SCORE: 0.7414678958749981\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GPk2qk6psuk2"},"source":["> **Results** - The optimised model scored well on out test set, scoring an AUC score of 0.75 . We will proceed to understand our model a bit better before making final critiques and recommendations."]},{"cell_type":"code","metadata":{"id":"rztkG23csuk3"},"source":["#DEFINING A FUNCTION TO VISUALISE MOST USED WORDS\n","def TF_IDF_most_used_words(category_string, data_series, palette, image_mask):\n","    #CHECKING OUT COMMON WORDS IN r/SuicideWatch USING TVEC\n","    tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n","    tvec_optimised.fit(data_series)\n","    #CREATING A DATAFRAME OF EXTRACTED WORDS\n","    created_df = pd.DataFrame(tvec_optimised.transform(data_series).todense(),\n","                              columns=tvec_optimised.get_feature_names())\n","    total_words = created_df.sum(axis=0)\n","    \n","    #<<<WORDCLOUD>>>\n","    #CREATING A LONG STRING OF WORDS FOR THE WORD CLOUD MODULE\n","    top_40_words = total_words.sort_values(ascending = False).head(40)\n","    top_40_words_df = pd.DataFrame(top_40_words)\n","    top_words_cloud_df = top_40_words_df.reset_index()\n","    top_words_cloud_df.columns = [\"words\", \"count\"]\n","    one_string_list = []\n","    for i in range(len(top_words_cloud_df)):\n","        one_string = (top_words_cloud_df[\"words\"][i] + \" \")* (top_words_cloud_df[\"count\"][i]).astype(int)\n","        one_string_list.append(one_string)\n","    long_string = \" \".join(string for string in one_string_list)\n","    #print(long_string)\n","    # CREATING A WORD CLOUD IMAGE\n","    mask = np.array(Image.open(image_mask))\n","    wordcloud = WordCloud(repeat=True, collocations=False,min_font_size=2, max_font_size= 80, max_words= 10000, background_color= \"white\",colormap= palette,  mask= mask).generate(long_string)\n","    # DISPLAY IT\n","    #plt.axis(\"off\")\n","    plt.figure(figsize = (20, 20), dpi=300)\n","    plt.title('\\n{}\\n'.format(category_string), fontsize=22)\n","    #plt.imshow(wordcloud, interpolation='bilinear') \n","    image_colors = ImageColorGenerator(mask) #THIS MAKES THE WORDCLOUD RESPOND TO THE COLOURS IN THE MASK\n","    plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.show()\n","    \n","    #<<<BARPLOT>>>\n","    #CREATING A FINAL DATAFRAME OF THE TOP 20 WORDS\n","    top_20_words = total_words.sort_values(ascending = False).head(20)\n","    top_20_words_df = pd.DataFrame(top_20_words, columns = [\"count\"])\n","    #PLOTTING THE COUNT OF THE TOP 20 WORDS\n","    sns.set_style(\"white\")\n","    plt.figure(figsize = (15, 8), dpi=300)\n","    ax = sns.barplot(y= top_20_words_df.index, x=\"count\", data=top_20_words_df, palette = palette)\n","    \n","    plt.xlabel(\"Count\", fontsize=9)\n","    plt.ylabel('Common Words in {}'.format(category_string), fontsize=9)\n","    plt.yticks(rotation=-5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"3FLFFYL6suk7","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"error","timestamp":1605065505071,"user_tz":-330,"elapsed":3007,"user":{"displayName":"RAJSWI LOCHAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZeLrSwGUN4QcS3rJjdls1KLtBAINGLXs1QdAP=s64","userId":"03455964087078711432"}},"outputId":"20075ff1-d474-4cce-dc9f-a2841852200a"},"source":["#CALLING THE FUNCTION ON OUR X FEATURES\n","TF_IDF_most_used_words(\"Words used by production model to identify r/SuicideWatch Posts\", model_data[\"megatext_clean\"], \"vlag_r\", image_mask=\"/content/drive/My Drive/ML mini project/assets/ending_mask_8.png\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b42fde08f076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#CALLING THE FUNCTION ON OUR X FEATURES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTF_IDF_most_used_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Words used by production model to identify r/SuicideWatch Posts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"megatext_clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vlag_r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/My Drive/ML mini project/assets/ending_mask_8.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-df5f62f4a8a6>\u001b[0m in \u001b[0;36mTF_IDF_most_used_words\u001b[0;34m(category_string, data_series, palette, image_mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#print(long_string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# CREATING A WORD CLOUD IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollocations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# DISPLAY IT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/ML mini project/assets/ending_mask_8.png'"]}]},{"cell_type":"code","metadata":{"id":"xJ7gvCqwsulB"},"source":["#SEEKING OUT FALSE NEGATIVES\n","\n","#SETTING UP A DATAFRAME WITH ONLY X_test POSTS\n","index_seek = list(X_test.index)\n","fn_explore = model_data.iloc[index_seek, :][[\"title\",\"selftext\",\"author\", \"is_suicide\"]]\n","fn_explore[\"predictions\"] = nb.predict(X_test_tvec)\n","\n","# SEEKING OUT OUR FALSE NEGATIVES\n","#is_suicide == 1 and predictions == 0\n","false_negs = fn_explore[fn_explore[\"is_suicide\"]==1][fn_explore[\"predictions\"]==0]\n","false_negs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLpIm5OtsulL"},"source":["#DEFINING A FUNCTION TO VISUALISE MOST USED WORDS\n","def barplot_most_used_words(category_string, data_series, palette):\n","    #CHECKING OUT COMMON WORDS IN r/SuicideWatch USING TVEC\n","    tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n","    tvec_optimised.fit(data_series)\n","    #CREATING A DATAFRAME OF EXTRACTED WORDS\n","    created_df = pd.DataFrame(tvec_optimised.transform(data_series).todense(),\n","                              columns=tvec_optimised.get_feature_names())\n","    total_words = created_df.sum(axis=0)\n","    \n","    #<<<BARPLOT>>>\n","    #CREATING A FINAL DATAFRAME OF THE TOP 20 WORDS\n","    top_20_words = total_words.sort_values(ascending = False).head(20)\n","    top_20_words_df = pd.DataFrame(top_20_words, columns = [\"count\"])\n","    #PLOTTING THE COUNT OF THE TOP 20 WORDS\n","    sns.set_style(\"white\")\n","    plt.figure(figsize = (15, 8), dpi=300)\n","    ax = sns.barplot(y= top_20_words_df.index, x=\"count\", data=top_20_words_df, palette = palette)\n","    plt.title('\\nTop Words used in {}\\n'.format(category_string), fontsize=22)\n","    plt.xlabel(\"Count\", fontsize=9)\n","    plt.ylabel('Common Words in {}'.format(category_string), fontsize=9)\n","    plt.yticks(rotation=-5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hF_LsVq9sulP"},"source":["#VISUALISING WORDS USED BY FALSE NEGATIVES\n","barplot_most_used_words(\"False Negatives\", false_negs[\"selftext\"], \"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ePu5zYyJsulV"},"source":["### 3.5 Model evaluation and possible future developments \n","\n"]},{"cell_type":"markdown","metadata":{"id":"_s8jIRJlsulW"},"source":["- **Top Words picked by TF-IDF Vectorizer** - It is important to note that the Vectorizer weighted \"feel\" higher than \"want\" although (as seen in our earlier EDA), \"want\" appeared more times than \"feel\" in suicide posts. This is probablly because TF-IDF \"penalises\" words that appear too much. Nonetheless, this set of words might be a good starting point to build other models that might be able to predict the presence of suicidal individuals in, say a subreddit for teachers. \n","\n","\n","- **Understanding our False Negatives** - In our barplot showing the top words in our false negatives, we can see many similar words to those that our model is prioritising. To improve our model, we need to use more robust textual analysis to lower the instances of false negatives and this will prevent individuals at risk of suicide from falling through the cracks.\n","\n","\n","- **Suicide without Depression** - One of the biggest limitations of this project is that we only looked at suicidal tendencies in the context of depression. It is important to note that suicides happen to people without depression too. Thus, future studies can focus on the wider population. \n","\n","\n","- **Double-Posting Phenomenon** - In our exploratory data analysis, we noticed more about 26 instances of individuals posting on both subreddits. The example we looked at was u/thathumbletrashcan posted on r/depression on March 4th that \"*I don't want to die, but I don't want to live anymore*\" . A day later, u/thathumbletrashcan visits the r/SuicideWatch forum and posts \"*I've finally grown the balls to fulfil my plan......all of you won't have to deal with me again*.\" Social psychologist Roy Baumeister postulates in his “Suicide as Escape from Self” theory thata person descends into the pit of self-extinction in six stages. Studying double-posting through the lens of Baumeister's theory might yield links to stage-based progression amongst our redditors.\n","\n","\n","- **Looking into Male Suicides** - According to Samaritans of Singapore, males account for more than 71% of all suicides in 2018. This is consistent with our EDA findings of male-signifiers in usernames. Future projects can dive deeper into suicide amongst males. This could possibly be relevant for companies in male-dominated, high-stress industries like oil/gas, banking and tech. \n","\n","\n","- **Complexity of Depression** - Depression is complex and layered. Future development on our model might be aided by working with someone with domain knowledge, who can help us with filtering out factors that might be linked to other conditions like self-harm/OCD/Anxiety?"]}]}